{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wells.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nalgo_df = pd.DataFrame(np.zeros((4,2)),columns=['train','test'],index=['Logistic','SGD','SVM','RF'])\\n\\nalgo_df.iloc[1][0], algo_df.iloc[1][1], SGD_clf = SGD_mhs(X_train_scaled,X_test_scaled,y_train,y_test)\\nalgo_df.iloc[0][0], algo_df.iloc[0][1], LR_clf = LR_mhs(X_train,X_test,y_train,y_test)\\nalgo_df.iloc[2][0], algo_df.iloc[2][1], SVM_clf = SVM_mhs(X_train_scaled,X_test_scaled,y_train,y_test)\\nalgo_df.iloc[3][0], algo_df.iloc[3][1], RF_clf = RF_mhs(X_train,X_test,y_train,y_test)\\nprint(algo_df)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load wells_classifiers.py\n",
    "\"\"\"\n",
    "Created on Sun May  1 12:07:20 2016\n",
    "\n",
    "@author: Michael\n",
    "\"\"\"\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Suppress FutureWarning\n",
    "def fxn():\n",
    "    warnings.warn(\"future\", FutureWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "def accuracy_score(X,y,clf):\n",
    "    scr = 0    \n",
    "    y_pred = clf.predict(X)\n",
    "    for i in range(0,len(y)):\n",
    "        if(y_pred[i]==y.iloc[i]):\n",
    "            scr+=1\n",
    "    acc = float(scr)/len(y)  \n",
    "    return acc\n",
    "\n",
    "#Performs cross-validation on clf, outputting each fold to screen\n",
    "def cross_val(X,y,clf,folds=5):\n",
    "    cv_scores = model_selection.cross_val_score(clf,X,y,cv=folds)\n",
    "    print(\"Cross-Validation with %i folds. Mean accuracy: %.4f\" % (folds,np.mean(cv_scores)))\n",
    "    \n",
    "#Creates a training and test, then runs logisticRegression on each single feature, \n",
    "#returning training and test accuracy for each feature\n",
    "def single_feat(X_train,X_test,y_train,y_test,clf):\n",
    "    acc_df = pd.DataFrame(np.zeros((X_train.shape[1],2)),columns=['train','test'])\n",
    "    for i in range(0,X_train.shape[1]):\n",
    "        clf.fit(X_train[:,i:i+1],y_train)\n",
    "        acc_df.iloc[i][0] = accuracy_score(X_train[:,i:i+1],y_train,clf)\n",
    "        acc_df.iloc[i][1] = accuracy_score(X_test[:,i:i+1],y_test,clf)\n",
    "    print(acc_df)\n",
    "    return acc_df\n",
    "\n",
    "# Scale features\n",
    "def scale_X(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train) \n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# GridSearch Parameters\n",
    "def grid_optimize(clf,param_grid,X,y):\n",
    "    cv = cross_validation.StratifiedShuffleSplit(y, n_iter=5, test_size=0.2)\n",
    "    grid = GridSearchCV(clf,param_grid,cv=cv)\n",
    "    grid.fit(X,y)\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "    return grid\n",
    "\n",
    "# Logistic Regression\n",
    "def LR_mhs(X_train,X_test,y_train,y_test):\n",
    "    LR_clf = linear_model.LogisticRegression()  \n",
    "    LR_param_grid = {'penalty':['l1','l2']}\n",
    "    LR_grid = grid_optimize(LR_clf,LR_param_grid,X_train,y_train)\n",
    "    LR_clf = linear_model.LogisticRegression(penalty=LR_grid.best_params_['penalty'])\n",
    "    LR_clf.fit(X_train,y_train)\n",
    "#    LogReg_acc = single_feat(X_train,X_test,y_train,y_test,LR_clf)\n",
    "    train_acc = accuracy_score(X_train,y_train,LR_clf)\n",
    "    test_acc = accuracy_score(X_test,y_test,LR_clf)\n",
    "    print(\"LR Classifier\")    \n",
    "    print(\"Training Accuracy: %.4f\" % train_acc)\n",
    "    print(\"Testing Accuracy: %.4f\" % test_acc)\n",
    "    return train_acc, test_acc, LR_clf\n",
    "    \n",
    "# SGDClassifier\n",
    "def SGD_mhs(X_train_scaled,X_test_scaled,y_train,y_test):\n",
    "    SGD_clf = linear_model.SGDClassifier(max_iter=1000,tol=1e-3)\n",
    "    SGD_param_grid = {'penalty':['l1','l2','elasticnet'],'loss':['hinge','log','squared_loss'],'alpha':[.000003,.00001,.00003,.0001,.0003,.001]}\n",
    "    SGD_grid = grid_optimize(SGD_clf,SGD_param_grid,X_train_scaled,y_train)\n",
    "    SGD_clf = linear_model.SGDClassifier(loss=SGD_grid.best_params_['loss'],penalty=SGD_grid.best_params_['penalty'],alpha=SGD_grid.best_params_['alpha'],max_iter=1000,tol=1e-3)\n",
    "#    SGD_acc = single_feat(X_train_scaled,X_test_scaled,y_train,y_test,SGD_clf)\n",
    "    SGD_clf.fit(X_train_scaled,y_train)\n",
    "    train_acc = accuracy_score(X_train_scaled,y_train,SGD_clf)\n",
    "    test_acc = accuracy_score(X_test_scaled,y_test,SGD_clf)\n",
    "    print(\"SGD Classifier\")    \n",
    "    print(\"Training Accuracy: %.4f\" % train_acc)\n",
    "    print(\"Testing Accuracy: %.4f\" % test_acc)\n",
    "    return train_acc, test_acc, SGD_clf\n",
    "\n",
    "# SVM with rbf kernel Classifier\n",
    "def SVM_mhs(X_train_scaled,X_test_scaled,y_train,y_test):\n",
    "    SVM_clf = svm.SVC(kernel='rbf')\n",
    "    C_range = [.01,300,10000000]\n",
    "    gamma_range = [.000000001,.0003,1000]\n",
    "   # SVM_param_grid = dict(gamma=gamma_range,C=C_range)\n",
    "   # SVM_grid = grid_optimize(SVM_clf,SVM_param_grid,X_train_scaled,y_train)\n",
    "   # SVM_clf = svm.SVC(gamma=SVM_grid.best_params_['gamma'],C=SVM_grid.best_params_['C'])\n",
    "    SVM_clf = svm.SVC(gamma=.0003,C=100) \n",
    "    SVM_clf.fit(X_train_scaled,y_train)\n",
    "    train_acc = accuracy_score(X_train_scaled,y_train,SVM_clf)\n",
    "    test_acc = accuracy_score(X_test_scaled,y_test,SVM_clf)\n",
    "    print(\"SVC Classifier with rbf kernel\")    \n",
    "    print(\"Training Accuracy: %.4f\" % train_acc)\n",
    "    print(\"Testing  Accuracy: %.4f\" % test_acc)\n",
    "    return train_acc, test_acc, SVM_clf\n",
    "\n",
    "def RF_mhs(X_train,X_test,y_train,y_test):\n",
    "    RF_clf = RandomForestClassifier(n_estimators = 10, max_depth = 5)  \n",
    "    RF_param_grid = {'max_features':[None,'log2','sqrt']}\n",
    "    RF_grid = grid_optimize(RF_clf,RF_param_grid,X_train,y_train)\n",
    "    RF_clf = RandomForestClassifier(n_estimators=10,max_depth=5,max_features=RF_grid.best_params_['max_features'])\n",
    "    RF_clf.fit(X_train,y_train)\n",
    "    train_acc = accuracy_score(X_train,y_train,RF_clf)\n",
    "    test_acc = accuracy_score(X_test,y_test,RF_clf)\n",
    "    print(\"RF Classifier\")    \n",
    "    print(\"Training Accuracy: %.4f\" % train_acc)\n",
    "    print(\"Testing Accuracy: %.4f\" % test_acc)\n",
    "    return train_acc, test_acc, RF_clf    \n",
    "\n",
    "# MAIN \n",
    "#Create training and test sets, scaled and not\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.35)\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "scal=[]\n",
    "for i in range(0,X.shape[1]):\n",
    "    if max(X.iloc[:,i])!=1:\n",
    "        scal.append(i)  \n",
    "X_train_scaled.iloc[:,scal], X_test_scaled.iloc[:,scal] = scale_X(X_train.iloc[:,scal],X_test.iloc[:,scal])\n",
    "del(scal)\n",
    "\n",
    "'''\n",
    "algo_df = pd.DataFrame(np.zeros((4,2)),columns=['train','test'],index=['Logistic','SGD','SVM','RF'])\n",
    "\n",
    "algo_df.iloc[1][0], algo_df.iloc[1][1], SGD_clf = SGD_mhs(X_train_scaled,X_test_scaled,y_train,y_test)\n",
    "algo_df.iloc[0][0], algo_df.iloc[0][1], LR_clf = LR_mhs(X_train,X_test,y_train,y_test)\n",
    "algo_df.iloc[2][0], algo_df.iloc[2][1], SVM_clf = SVM_mhs(X_train_scaled,X_test_scaled,y_train,y_test)\n",
    "algo_df.iloc[3][0], algo_df.iloc[3][1], RF_clf = RF_mhs(X_train,X_test,y_train,y_test)\n",
    "print(algo_df)\n",
    "'''\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nn = pd.get_dummies(y_train)\n",
    "y_test_nn = pd.get_dummies(y_test)\n",
    "number_of_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic fully connected model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add fully connected layers with a ReLU activation function\n",
    "model.add(layers.Dense(units=200, activation='relu', input_shape=(number_of_features,)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units=200, activation='tanh'))\n",
    "\n",
    "\n",
    "# Output layer for multi-class: softmax function\n",
    "model.add(layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile neural network\n",
    "model.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "                optimizer='adam', # Root Mean Square Propagation\n",
    "                metrics=['accuracy']) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = model.fit(X_train, # Features\n",
    "                      y_train_nn, # Target vector\n",
    "                      epochs=250, # Number of epochs\n",
    "                      verbose=0, # Print description after each epoch\n",
    "                      batch_size=1000, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test_nn)) # Data for evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38610 samples, validate on 20790 samples\n",
      "Epoch 1/100\n",
      "38610/38610 [==============================] - 1s 21us/step - loss: 1.1101 - acc: 0.4343 - val_loss: 1.0050 - val_acc: 0.5414\n",
      "Epoch 2/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.9816 - acc: 0.5720 - val_loss: 0.9517 - val_acc: 0.5924\n",
      "Epoch 3/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.9350 - acc: 0.6000 - val_loss: 0.9178 - val_acc: 0.6039\n",
      "Epoch 4/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.9082 - acc: 0.6056 - val_loss: 0.8976 - val_acc: 0.6053\n",
      "Epoch 5/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.8906 - acc: 0.6077 - val_loss: 0.8821 - val_acc: 0.6095\n",
      "Epoch 6/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.8770 - acc: 0.6099 - val_loss: 0.8706 - val_acc: 0.6110\n",
      "Epoch 7/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8671 - acc: 0.6116 - val_loss: 0.8627 - val_acc: 0.6108\n",
      "Epoch 8/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8604 - acc: 0.6121 - val_loss: 0.8566 - val_acc: 0.6121\n",
      "Epoch 9/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8551 - acc: 0.6126 - val_loss: 0.8524 - val_acc: 0.6107\n",
      "Epoch 10/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8509 - acc: 0.6133 - val_loss: 0.8487 - val_acc: 0.6117\n",
      "Epoch 11/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8473 - acc: 0.6142 - val_loss: 0.8462 - val_acc: 0.6116\n",
      "Epoch 12/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8444 - acc: 0.6148 - val_loss: 0.8438 - val_acc: 0.6121\n",
      "Epoch 13/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8419 - acc: 0.6157 - val_loss: 0.8420 - val_acc: 0.6123\n",
      "Epoch 14/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8409 - acc: 0.6158 - val_loss: 0.8408 - val_acc: 0.6132\n",
      "Epoch 15/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8412 - acc: 0.6155 - val_loss: 0.8399 - val_acc: 0.6126\n",
      "Epoch 16/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8379 - acc: 0.6167 - val_loss: 0.8384 - val_acc: 0.6129\n",
      "Epoch 17/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8371 - acc: 0.6166 - val_loss: 0.8371 - val_acc: 0.6133\n",
      "Epoch 18/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8349 - acc: 0.6169 - val_loss: 0.8359 - val_acc: 0.6130\n",
      "Epoch 19/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8337 - acc: 0.6175 - val_loss: 0.8344 - val_acc: 0.6138\n",
      "Epoch 20/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8325 - acc: 0.6172 - val_loss: 0.8327 - val_acc: 0.6146\n",
      "Epoch 21/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8306 - acc: 0.6186 - val_loss: 0.8310 - val_acc: 0.6140\n",
      "Epoch 22/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8295 - acc: 0.6189 - val_loss: 0.8299 - val_acc: 0.6143\n",
      "Epoch 23/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8279 - acc: 0.6178 - val_loss: 0.8289 - val_acc: 0.6135\n",
      "Epoch 24/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8267 - acc: 0.6189 - val_loss: 0.8279 - val_acc: 0.6134\n",
      "Epoch 25/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8253 - acc: 0.6193 - val_loss: 0.8265 - val_acc: 0.6140\n",
      "Epoch 26/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8279 - acc: 0.6220 - val_loss: 0.8258 - val_acc: 0.6144\n",
      "Epoch 27/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8242 - acc: 0.6196 - val_loss: 0.8251 - val_acc: 0.6136\n",
      "Epoch 28/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8230 - acc: 0.6180 - val_loss: 0.8235 - val_acc: 0.6141\n",
      "Epoch 29/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8216 - acc: 0.6189 - val_loss: 0.8221 - val_acc: 0.6140\n",
      "Epoch 30/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8201 - acc: 0.6192 - val_loss: 0.8207 - val_acc: 0.6149\n",
      "Epoch 31/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8199 - acc: 0.6194 - val_loss: 0.8204 - val_acc: 0.6152\n",
      "Epoch 32/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8190 - acc: 0.6194 - val_loss: 0.8190 - val_acc: 0.6145\n",
      "Epoch 33/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8172 - acc: 0.6193 - val_loss: 0.8178 - val_acc: 0.6150\n",
      "Epoch 34/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8168 - acc: 0.6192 - val_loss: 0.8167 - val_acc: 0.6159\n",
      "Epoch 35/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8164 - acc: 0.6211 - val_loss: 0.8167 - val_acc: 0.6169\n",
      "Epoch 36/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8147 - acc: 0.6210 - val_loss: 0.8162 - val_acc: 0.6173\n",
      "Epoch 37/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8139 - acc: 0.6209 - val_loss: 0.8146 - val_acc: 0.6166\n",
      "Epoch 38/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8136 - acc: 0.6201 - val_loss: 0.8144 - val_acc: 0.6160\n",
      "Epoch 39/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8126 - acc: 0.6198 - val_loss: 0.8137 - val_acc: 0.6155\n",
      "Epoch 40/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8118 - acc: 0.6202 - val_loss: 0.8128 - val_acc: 0.6157\n",
      "Epoch 41/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.8118 - acc: 0.6198 - val_loss: 0.8122 - val_acc: 0.6161\n",
      "Epoch 42/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8110 - acc: 0.6207 - val_loss: 0.8113 - val_acc: 0.6170\n",
      "Epoch 43/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8101 - acc: 0.6212 - val_loss: 0.8111 - val_acc: 0.6166\n",
      "Epoch 44/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8095 - acc: 0.6205 - val_loss: 0.8101 - val_acc: 0.6168\n",
      "Epoch 45/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8087 - acc: 0.6210 - val_loss: 0.8095 - val_acc: 0.6173\n",
      "Epoch 46/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8087 - acc: 0.6207 - val_loss: 0.8089 - val_acc: 0.6177\n",
      "Epoch 47/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8079 - acc: 0.6212 - val_loss: 0.8083 - val_acc: 0.6166\n",
      "Epoch 48/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8071 - acc: 0.6220 - val_loss: 0.8078 - val_acc: 0.6172\n",
      "Epoch 49/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8070 - acc: 0.6215 - val_loss: 0.8072 - val_acc: 0.6175\n",
      "Epoch 50/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8060 - acc: 0.6217 - val_loss: 0.8070 - val_acc: 0.6170\n",
      "Epoch 51/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8059 - acc: 0.6215 - val_loss: 0.8068 - val_acc: 0.6167\n",
      "Epoch 52/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8058 - acc: 0.6214 - val_loss: 0.8066 - val_acc: 0.6161\n",
      "Epoch 53/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8053 - acc: 0.6210 - val_loss: 0.8057 - val_acc: 0.6177\n",
      "Epoch 54/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8048 - acc: 0.6216 - val_loss: 0.8052 - val_acc: 0.6176\n",
      "Epoch 55/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8043 - acc: 0.6221 - val_loss: 0.8051 - val_acc: 0.6172\n",
      "Epoch 56/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8045 - acc: 0.6213 - val_loss: 0.8042 - val_acc: 0.6184\n",
      "Epoch 57/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8030 - acc: 0.6221 - val_loss: 0.8040 - val_acc: 0.6177\n",
      "Epoch 58/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8022 - acc: 0.6230 - val_loss: 0.8035 - val_acc: 0.6180\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8023 - acc: 0.6225 - val_loss: 0.8039 - val_acc: 0.6176\n",
      "Epoch 60/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8026 - acc: 0.6227 - val_loss: 0.8038 - val_acc: 0.6181\n",
      "Epoch 61/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8019 - acc: 0.6218 - val_loss: 0.8035 - val_acc: 0.6173\n",
      "Epoch 62/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.8017 - acc: 0.6222 - val_loss: 0.8037 - val_acc: 0.6178\n",
      "Epoch 63/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8016 - acc: 0.6232 - val_loss: 0.8030 - val_acc: 0.6177\n",
      "Epoch 64/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8013 - acc: 0.6224 - val_loss: 0.8028 - val_acc: 0.6179\n",
      "Epoch 65/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8002 - acc: 0.6230 - val_loss: 0.8026 - val_acc: 0.6170\n",
      "Epoch 66/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7996 - acc: 0.6227 - val_loss: 0.8018 - val_acc: 0.6176\n",
      "Epoch 67/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7993 - acc: 0.6232 - val_loss: 0.8013 - val_acc: 0.6176\n",
      "Epoch 68/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7995 - acc: 0.6227 - val_loss: 0.8008 - val_acc: 0.6179\n",
      "Epoch 69/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7990 - acc: 0.6235 - val_loss: 0.8011 - val_acc: 0.6173\n",
      "Epoch 70/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7987 - acc: 0.6228 - val_loss: 0.8007 - val_acc: 0.6185\n",
      "Epoch 71/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7983 - acc: 0.6245 - val_loss: 0.7999 - val_acc: 0.6191\n",
      "Epoch 72/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.7975 - acc: 0.6241 - val_loss: 0.7999 - val_acc: 0.6182\n",
      "Epoch 73/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.7979 - acc: 0.6240 - val_loss: 0.7999 - val_acc: 0.6188\n",
      "Epoch 74/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7969 - acc: 0.6249 - val_loss: 0.7995 - val_acc: 0.6210\n",
      "Epoch 75/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7958 - acc: 0.6261 - val_loss: 0.7998 - val_acc: 0.6183\n",
      "Epoch 76/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7975 - acc: 0.6233 - val_loss: 0.7999 - val_acc: 0.6178\n",
      "Epoch 77/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7956 - acc: 0.6259 - val_loss: 0.7999 - val_acc: 0.6195\n",
      "Epoch 78/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7974 - acc: 0.6281 - val_loss: 0.7985 - val_acc: 0.6196\n",
      "Epoch 79/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7970 - acc: 0.6227 - val_loss: 0.7986 - val_acc: 0.6189\n",
      "Epoch 80/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7959 - acc: 0.6248 - val_loss: 0.7985 - val_acc: 0.6194\n",
      "Epoch 81/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7956 - acc: 0.6247 - val_loss: 0.7987 - val_acc: 0.6188\n",
      "Epoch 82/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7956 - acc: 0.6246 - val_loss: 0.7984 - val_acc: 0.6190\n",
      "Epoch 83/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7955 - acc: 0.6260 - val_loss: 0.7986 - val_acc: 0.6193\n",
      "Epoch 84/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.7949 - acc: 0.6248 - val_loss: 0.7990 - val_acc: 0.6182\n",
      "Epoch 85/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7943 - acc: 0.6256 - val_loss: 0.7984 - val_acc: 0.6196\n",
      "Epoch 86/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.7947 - acc: 0.6250 - val_loss: 0.7981 - val_acc: 0.6206\n",
      "Epoch 87/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7966 - acc: 0.6261 - val_loss: 0.7969 - val_acc: 0.6241\n",
      "Epoch 88/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7958 - acc: 0.6288 - val_loss: 0.7965 - val_acc: 0.6223\n",
      "Epoch 89/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7942 - acc: 0.6257 - val_loss: 0.7976 - val_acc: 0.6191\n",
      "Epoch 90/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7934 - acc: 0.6262 - val_loss: 0.7972 - val_acc: 0.6212\n",
      "Epoch 91/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.8028 - acc: 0.6313 - val_loss: 0.7947 - val_acc: 0.6269\n",
      "Epoch 92/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7947 - acc: 0.6269 - val_loss: 0.7969 - val_acc: 0.6211\n",
      "Epoch 93/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7927 - acc: 0.6274 - val_loss: 0.7977 - val_acc: 0.6193\n",
      "Epoch 94/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7935 - acc: 0.6256 - val_loss: 0.7978 - val_acc: 0.6195\n",
      "Epoch 95/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7932 - acc: 0.6276 - val_loss: 0.7964 - val_acc: 0.6207\n",
      "Epoch 96/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7925 - acc: 0.6291 - val_loss: 0.7960 - val_acc: 0.6218\n",
      "Epoch 97/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7920 - acc: 0.6268 - val_loss: 0.7968 - val_acc: 0.6196\n",
      "Epoch 98/100\n",
      "38610/38610 [==============================] - 1s 16us/step - loss: 0.7934 - acc: 0.6266 - val_loss: 0.7963 - val_acc: 0.6202\n",
      "Epoch 99/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7912 - acc: 0.6282 - val_loss: 0.7949 - val_acc: 0.6238\n",
      "Epoch 100/100\n",
      "38610/38610 [==============================] - 1s 15us/step - loss: 0.7924 - acc: 0.6338 - val_loss: 0.7912 - val_acc: 0.6287\n"
     ]
    }
   ],
   "source": [
    "#Dropout model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add fully connected layers with a ReLU activation function\n",
    "model.add(Dense(64, activation='relu', input_shape=(number_of_features,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Output layer for multi-class: softmax function\n",
    "model.add(layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile neural network\n",
    "model.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "                optimizer='adam', # Root Mean Square Propagation\n",
    "                metrics=['accuracy']) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = model.fit(X_train, # Features\n",
    "                      y_train_nn, # Target vector\n",
    "                      epochs=100, # Number of epochs\n",
    "                      verbose=1, # Print description after each epoch\n",
    "                      batch_size=1000, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test_nn)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa32c2da7f0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
